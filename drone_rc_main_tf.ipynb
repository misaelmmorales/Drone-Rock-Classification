{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning-based automated rock classification via high-resolution drone-captured core sample imagery\n",
    "***\n",
    "### Domenico M. Crisafulli, Misael M. Morales, and Carlos Torres-Verdin\n",
    "#### The University of Texas at Austin, 2024\n",
    "***\n",
    "\n",
    "## Build and Train NN-classifier\n",
    "| Class             | OLD   | New   |\n",
    "| ---               | ---   | ---   |\n",
    "| Background        | 0     | 0     |\n",
    "| Sandstone type 1  | 1     | 2     |\n",
    "| Shaly Rock        | 2     | 3     |\n",
    "| Sandstone type 2  | 3     | 4     |\n",
    "| Carbonate         | 4     | 5     |\n",
    "| Shale             | 5     | 6     |\n",
    "| Sandstone type 3  | 6     | 7     |\n",
    "| Box               | 10    | 1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "TF version: 2.16.1 | # Device(s) available: 2\n",
      "TF Built with CUDA? True | CUDA: 12.0 | cuDNN: 8\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n",
      " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras import layers, losses, optimizers\n",
    "from keras.activations import gelu\n",
    "from keras.applications.resnet import ResNet50, preprocess_input\n",
    "\n",
    "def check_tf_gpu():\n",
    "    sys_info = tf.sysconfig.get_build_info()\n",
    "    version, cuda, cudnn = tf.__version__, sys_info[\"cuda_version\"], sys_info[\"cudnn_version\"]\n",
    "    count = len(tf.config.experimental.list_physical_devices())\n",
    "    name  = [device.name for device in tf.config.experimental.list_physical_devices('GPU')]\n",
    "    print('-'*60)\n",
    "    print('----------------------- VERSION INFO -----------------------')\n",
    "    print('TF version: {} | # Device(s) available: {}'.format(version, count))\n",
    "    print('TF Built with CUDA? {} | CUDA: {} | cuDNN: {}'.format(tf.test.is_built_with_cuda(), cuda, cudnn))\n",
    "    print(tf.config.list_physical_devices()[0],'\\n', tf.config.list_physical_devices()[1])\n",
    "    print('-'*60+'\\n')\n",
    "    return None\n",
    "\n",
    "check_tf_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, use_bias=False):\n",
    "        x = layers.Conv2D(num_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding=\"same\", \n",
    "                          use_bias=use_bias, kernel_initializer=keras.initializers.HeNormal())(block_input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    def DilatedSpatialPyramidPooling(dspp_input):\n",
    "        dims = dspp_input.shape\n",
    "        x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "        x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "        out_pool = layers.UpSampling2D(\n",
    "            size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "        out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "        out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "        out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "        out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "        x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "        output = convolution_block(x, kernel_size=1)\n",
    "        return output\n",
    "\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    \n",
    "    preprocessed = preprocess_input(model_input)\n",
    "    resnet50 = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=preprocessed)\n",
    "\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(input_image, patch_h=252, patch_w=252, pad=4, channel_axis=-1):\n",
    "    if channel_axis is not None:\n",
    "        h, w, c = input_image.shape\n",
    "    else:\n",
    "        h, w = input_image.shape\n",
    "    patches = []\n",
    "    for i in range(0, h, patch_h):\n",
    "        for j in range(0, w, patch_w):\n",
    "            if channel_axis is not None:\n",
    "                patch = input_image[i:i+patch_h, j:j+patch_w, :]\n",
    "                patch = np.pad(patch, ((pad,pad),(pad,pad),(0,0)))\n",
    "            else:\n",
    "                patch = input_image[i:i+patch_h, j:j+patch_w]\n",
    "                patch = np.pad(patch, ((pad,pad),(pad,pad)))\n",
    "            patches.append(patch)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names_pictures = []\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for f in files:\n",
    "        if (f.endswith('.jpg') or f.endswith('.JPG')) and f.startswith('DJI'):\n",
    "            x_names_pictures.append(os.path.join(root, f))\n",
    "\n",
    "y_names_masks = []\n",
    "for root, dirs, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.mat') and file.startswith('img_'):\n",
    "            y_names_masks.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 17/17 [00:18<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3024, 4032, 3) (15, 3024, 4032)\n"
     ]
    }
   ],
   "source": [
    "x_pictures = []\n",
    "y_masks    = []\n",
    "for i in tqdm(range(len(x_names_pictures)), desc='Loading images'):\n",
    "    y = sio.loadmat(y_names_masks[i],simplify_cells=True)\n",
    "    if 'AA' in y.keys():\n",
    "        y_masks.append(y['AA'])\n",
    "        x = np.array(Image.open(x_names_pictures[i]).convert('RGB'))\n",
    "        x_pictures.append(x)\n",
    "\n",
    "X_data = np.array(x_pictures)\n",
    "y_data = np.array(y_masks)\n",
    "print(X_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patching images: 100%|██████████| 15/15 [00:00<00:00, 64.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 512, 512, 3) (720, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 504\n",
    "PADDING = 4\n",
    "PADDED_IMAGE = IMAGE_SIZE+PADDING*2\n",
    "\n",
    "x_data_patched = []\n",
    "y_data_patched = []\n",
    "for i in tqdm(range(X_data.shape[0]), desc='Patching images'):\n",
    "    x = patch_image(X_data[i], patch_h=IMAGE_SIZE, patch_w=IMAGE_SIZE, pad=PADDING)\n",
    "    x_data_patched.append(x)\n",
    "    y = patch_image(y_data[i], patch_h=IMAGE_SIZE, patch_w=IMAGE_SIZE, pad=PADDING, channel_axis=None)\n",
    "    y_data_patched.append(y)\n",
    "    \n",
    "x_data_patched = np.array(x_data_patched).reshape(-1, PADDED_IMAGE, PADDED_IMAGE, 3)\n",
    "y_data_patched = np.array(y_data_patched).reshape(-1, PADDED_IMAGE, PADDED_IMAGE, 1)\n",
    "print(x_data_patched.shape, y_data_patched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nonzero Filter: 100%|██████████| 720/720 [00:00<00:00, 10669.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 512, 512, 3) (273, 512, 512, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "for i in tqdm(range(y_data_patched.shape[0]), desc='Nonzero Filter'):\n",
    "    m = y_data_patched[i]\n",
    "    p = np.sum(m)\n",
    "    if p > 10000:\n",
    "        idx.append(i)\n",
    "\n",
    "x_data_nonzero = x_data_patched[idx]\n",
    "y_data_nonzero = y_data_patched[idx]\n",
    "print(x_data_nonzero.shape, y_data_nonzero.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X - train: (250, 512, 512, 3) | test: (23, 512, 512, 3)\n",
      "y - train: (250, 512, 512, 1) | test: (23, 512, 512, 1)\n",
      "Labels - train: [0 1 2 3 4 5 6] | test: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.choice(x_data_nonzero.shape[0], size=x_data_nonzero.shape[0], replace=False)\n",
    "n_train = 250\n",
    "\n",
    "X_train = x_data_nonzero[rand_idx[:n_train]]/255\n",
    "y_train = y_data_nonzero[rand_idx[:n_train]]\n",
    "\n",
    "X_test = x_data_nonzero[rand_idx[n_train:]]/255\n",
    "y_test = y_data_nonzero[rand_idx[n_train:]]\n",
    "\n",
    "y_train[y_train==255] = 5\n",
    "y_test[y_test==255] = 5\n",
    "\n",
    "print('X - train: {} | test: {}'.format(X_train.shape, X_test.shape))\n",
    "print('y - train: {} | test: {}'.format(y_train.shape, y_test.shape))\n",
    "print('Labels - train: {} | test: {}'.format(np.unique(y_train), np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 15, figsize=(25,4), sharex=True, sharey=True)\n",
    "for j in range(15):\n",
    "    ax1, ax2 = axs[0,j], axs[1,j]\n",
    "    ax1.imshow(X_train[j])\n",
    "    ax2.imshow(y_train[j])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 21:49:43.636467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.636590: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.636624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.840276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.840355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.840364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-23 21:49:43.840406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-23 21:49:43.840421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21770 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 11,853,638\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721789401.909377   83246 service.cc:145] XLA service 0x7fe560003d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1721789401.909429   83246 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-07-23 21:50:02.430955: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-23 21:50:04.135239: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    }
   ],
   "source": [
    "model = DeeplabV3Plus(image_size=512, num_classes=6)\n",
    "print('# parameters: {:,}'.format(model.count_params()))\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "criterion = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=criterion, metrics=['accuracy', 'mse'])\n",
    "\n",
    "start = time()\n",
    "fit = model.fit(X_train, y_train,\n",
    "                batch_size       = 8, \n",
    "                epochs           = 100, \n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                verbose          = 1)\n",
    "print('-'*30+'\\n'+'Training time: {:.2f} minutes'.format((time()-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(fit.history['loss'], ls='--', label='train loss')\n",
    "ax1.plot(fit.history['val_loss'], ls='--', label='val loss')\n",
    "ax2.plot(fit.history['accuracy'], label='train accuracy', color='red')\n",
    "ax2.plot(fit.history['val_accuracy'], label='val accuracy', color='green')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax1.grid(True, which='both')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train, verbose=0)\n",
    "y_test_pred  = model.predict(X_test, verbose=0)\n",
    "print('Pred - train: {} | test: {}'.format(y_train_pred.shape, y_test_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 15, figsize=(20,5), sharex=True, sharey=True)\n",
    "for j in range(15):\n",
    "    ax1, ax2, ax3 = axs[0,j], axs[1,j], axs[2,j]\n",
    "    ax1.imshow(X_train[j])\n",
    "    ax2.imshow(y_train[j])\n",
    "    ax3.imshow(y_train_pred[j])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load('data/x_images.npy')\n",
    "y_data = np.load('data/y_images.npy')\n",
    "print('X: {} | y: {}'.format(X_data.shape, y_data.shape))\n",
    "\n",
    "idx = np.random.choice(range(len(X_data)), len(X_data), replace=False)\n",
    "n_train = int(len(idx) * 0.8)\n",
    "\n",
    "X_train, y_train = X_data[idx[:n_train]], y_data[idx[:n_train]]\n",
    "X_test,  y_test  = X_data[idx[n_train:]], y_data[idx[n_train:]]\n",
    "print('X - train: {} | test: {}'.format(X_train.shape, X_test.shape))\n",
    "print('y - train: {} | test: {}'.format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RockClassification(image_size:int=512, out_channels:int=1):\n",
    "    \n",
    "    def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, use_bias=False):\n",
    "        x = layers.Conv2D(num_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding=\"same\", \n",
    "                          use_bias=use_bias, kernel_initializer=keras.initializers.HeNormal())(block_input)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = gelu(x)\n",
    "        return x\n",
    "\n",
    "    def DilatedSpatialPyramidPooling(dspp_input, num_filters=256):\n",
    "        dims = dspp_input.shape\n",
    "        x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "        x = convolution_block(x, num_filters, kernel_size=1, use_bias=True)\n",
    "        out_pool = layers.UpSampling2D(\n",
    "            size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\")(x)\n",
    "        out_1 = convolution_block(dspp_input, num_filters, kernel_size=1, dilation_rate=1)\n",
    "        out_6 = convolution_block(dspp_input, num_filters, kernel_size=3, dilation_rate=6)\n",
    "        out_12 = convolution_block(dspp_input, num_filters, kernel_size=3, dilation_rate=12)\n",
    "        out_18 = convolution_block(dspp_input, num_filters, kernel_size=3, dilation_rate=18)\n",
    "        x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "        output = convolution_block(x, num_filters, kernel_size=1)\n",
    "        return output\n",
    "    \n",
    "    input = keras.Input(shape=(image_size, image_size, 1))\n",
    "    _ = DilatedSpatialPyramidPooling(input, 256)\n",
    "    _ = convolution_block(_, 128)\n",
    "    _ = convolution_block(_, 64)\n",
    "    _ = convolution_block(_, 32)\n",
    "    _ = convolution_block(_, 16)\n",
    "    output = layers.Conv2D(out_channels, kernel_size=(3,3), padding='same')(_)\n",
    "\n",
    "    return Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RockClassification()\n",
    "print('# params: {:,}'.format(model.count_params()))\n",
    "model.compile(optimizer=optimizers.Adam(1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "fit = model.fit(X_train, y_train,\n",
    "                batch_size       = 4,\n",
    "                epochs           = 10,\n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                verbose          = 1)\n",
    "\n",
    "model.save_weights('rockClassification.weights.h5')\n",
    "pd.DataFrame(fit.history).to_csv('fit_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeplabV3Plus(image_size=512, num_classes=1)\n",
    "print('# params: {:,}'.format(model.count_params()))\n",
    "model.compile(optimizer=optimizers.AdamW(1e-3, 4e-6), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "fit = model.fit(X_train, y_train,\n",
    "                batch_size       = 8,\n",
    "                epochs           = 10,\n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                verbose          = 1)\n",
    "\n",
    "model.save_weights('DeeplabV3Plus.weights.h5')\n",
    "pd.DataFrame(fit.history).to_csv('DeeplabV3Plus_fit_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RockClassification()\n",
    "model.load_weights('rockClassification.weights.h5')\n",
    "print('# params: {:,}'.format(model.count_params()))\n",
    "\n",
    "losses = pd.read_csv('fit_history.csv')\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(losses.index, losses.accuracy, ls='-', marker='o', label='Accuracy')\n",
    "plt.plot(losses.index, losses.val_accuracy, ls='--', marker='.', label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(facecolor='lightgrey', edgecolor='k')\n",
    "plt.grid(True, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeplabV3Plus(image_size=512, num_classes=1)\n",
    "model.load_weights('DeeplabV3Plus.weights.h5')\n",
    "print('# params: {:,}'.format(model.count_params()))\n",
    "\n",
    "losses = pd.read_csv('DeeplabV3Plus_fit_history.csv')\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(losses.index, losses.accuracy, ls='-', marker='o', label='Accuracy')\n",
    "plt.plot(losses.index, losses.val_accuracy, ls='--', marker='.', label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(facecolor='lightgrey', edgecolor='k')\n",
    "plt.grid(True, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train[:6], verbose=0).round()\n",
    "y_test_pred  = model.predict(X_test[:6], verbose=0).round()\n",
    "print('Pred - train: {} | test: {}'.format(y_train_pred.shape, y_test_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 6, figsize=(15,5), sharex=True, sharey=True)\n",
    "for j in range(6):\n",
    "    ax1, ax2, ax3 = axs[0,j], axs[1,j], axs[2,j]\n",
    "    im1 = ax1.imshow(X_train[j])\n",
    "    im2 = ax2.imshow(y_train[j])\n",
    "    im3 = ax3.imshow(y_train_pred[j])\n",
    "    [a.set(xticks=[], yticks=[]) for a in [ax1, ax2, ax3]]\n",
    "    [plt.colorbar(i, pad=0.04, fraction=0.046) for i in [im1,im2,im3]]\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
